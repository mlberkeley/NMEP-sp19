{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn explore the topic of SVD in terms of the bias/variance tradeoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(0)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and review your slides/notes on SVD. Also, feel free to ask around. In short, an SVD (Singular Value Decomposition) of a matrix is a low-rank approximation of that matrix. What does this mean? Suppose the original matrix was of rank $R$, and we are trying to reduce it to rank $K$. SVD is the result of taking this list of $R$ unique vectors and approximating them as a linear combination of $K$ unique vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about this is geometrically - in SVD: $$ A \\approx U \\Sigma V^{T} $$ Suppose we multiply a vector $z$ with $A$. $$ A v \\approx U \\Sigma V^{T} z$$ We can think of this as a 3-step process: first, $V^{T}$ hits $z$ and *rotates* it into its eigenbasis (don't worry if you don't know what that is - all you should get out is that this first step is a rotation. Remember, $U$ and $V$ are orthonormal, so they don't change the magnitude of $z$, they only rotate). Then in step 2, $\\Sigma$ hits $z$ and stretches it along the principal axes - remember, $\\Sigma$ is a diagonal matrix, so what's happening here is that each element of the newly rotated $z$ is being scaled by a certain amount, the singular values. Finally in step 3, $U$ hits $z$ and rotates back into $z$'s *original* basis. Here's a visual of this whole process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/IM6Fn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how SVD can help in terms of bias/variance. We'll be using the MNIST dataset. As a reminder, each image is a 28x28 black and white image labeled a number 0-9. Keep this information in the back of your mind, thinking especially about bias-variance, as we walk you through the effect of using SVD on this simple classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a linear model. Reshape the X datasets appropriately for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a linear regressor and fit it to the training set. Hint: make sure your y_train is properly featurized. What does it look like right now? How should we be encoding categorical data such as this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the accuracies on both the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice something curious - train accuracy is lower than test accuracy?! What is happening here is a severe case of underfitting - the model is not complex enough (indeed, for an image recognition task, a linear model is very simple). Thus, the scores will be very similar - however here the fact that test set performance is higher than training set performance is just an anomaly, due to variance. Proper cross validation would solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is essentially the same as SVD. In PCA, the \"principal components\" are simply the singular vectors found in SVD. Apply PCA reduction to our data, keeping only the 100 most important dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with this new data, train a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate on train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we notice? Well the train accuracy certainly decreased, but the test accuracy increased! (not by much - a different dataset would have proved this point better, but for ease of use we stick with a linear MNIST example). So let's recap: before we had 28x28=784 components, and after reducing our data to 100 components our test accuracy increased by train accuracy decreased. This is a very important lesson in bias-variance. In the section below, explain very clearly how this demonstrates the bias-variance tradeoff, and what influences increased bias and what influences increased variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
